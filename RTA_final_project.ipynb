{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75af87f8",
   "metadata": {},
   "source": [
    "## Real Time Analytics Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a19fbb",
   "metadata": {},
   "source": [
    "<b> CryptoStreamLab: Real-Time Analysis of Cryptocurrency Markets Using Streaming Data </b> \\\n",
    "An Exploration of Real-Time Price and Volume Analysis with Spark, Kafka, and Streamlit for Enhanced Trading Strategies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848fe8b",
   "metadata": {},
   "source": [
    "<b> Author:\\\n",
    "    1. Cuong Vo - 131116\\\n",
    "    2. Trang Linh Nguyen - 131036\\\n",
    "    3. Aisel Akhmedova - 131008 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5a1fc",
   "metadata": {},
   "source": [
    "Tech Stack:\\\n",
    "    **1. Spark:** 3.5.0\\\n",
    "    **2. Python:** 3.10.12\\\n",
    "    **3. OS:** WSL Linux\\\n",
    "    **4. Streamlit:** \\\n",
    "    **5. Scala:** 2.12.18\\\n",
    "    **6. Java OpenJDK 64-Bit Server VM:** 11.0.25 \\\n",
    "    **7. Spark-submit:** org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6bbed",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This project explores the development of a real-time cryptocurrency trading system by leveraging modern streaming technologies and technical analysis. Using **Apache Kafka** for data ingestion, **Apache Spark Structured Streaming** for real-time processing, and **Streamlit** for live visualization, the system continuously streams market data from the **Binance API**. A trading strategy is implemented based on the **Relative Strength Index (RSI)** and **moving average crossovers**, generating buy, sell, or hold signals in real-time. The architecture supports immediate signal evaluation and portfolio adjustment, simulating trades using live data. The project demonstrates how scalable streaming tools can be combined with financial analytics to build a responsive and testable trading platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9677b1",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2d266",
   "metadata": {},
   "source": [
    "The dataset consists of real-time cryptocurrency market data streamed from Binance. Each record includes the following fields:\n",
    "\n",
    "- **Ticker**: The symbol representing the cryptocurrency pair (e.g., BTCUSDT).\n",
    "- **Timestamp**: The date and time when the data was recorded.\n",
    "- **Open**: The opening price of the cryptocurrency for the given time interval.\n",
    "- **Close**: The closing price of the cryptocurrency for the given time interval.\n",
    "- **Price**: The current price at the time of data capture.\n",
    "- **Volume**: The total trading volume of the cryptocurrency during the interval.\n",
    "\n",
    "This data enables real-time analysis of price movements and trading activity for various cryptocurrencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f9117",
   "metadata": {},
   "source": [
    "## Kafka Server setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6dbb3",
   "metadata": {},
   "source": [
    "Kafka Server is created on <b>localhost:9092</b> on WSL Linux Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b8e51",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ sudo systemctl daemon-reload\n",
    "$ sudo systemctl start zookeeper\n",
    "$ sudo systemctl start kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e6559",
   "metadata": {},
   "source": [
    "To check status of Kafka Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b68a70",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ sudo systemctl status kafka "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d5c25b",
   "metadata": {},
   "source": [
    "## Streaming Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8a2d70",
   "metadata": {},
   "source": [
    "### Create Kafka topic\n",
    "In this part, using Linux to create Kafka topic name StreamQuant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ec7a2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ cd /usr/local/kafka \n",
    "$ bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --topic StreamQuant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b0b853",
   "metadata": {},
   "source": [
    "Check for topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5f114",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ bin/kafka-topics.sh --list --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527df6d",
   "metadata": {},
   "source": [
    "### Create Kafka Procedure\n",
    "In this part, combined with the API key from Binance, we pull the data from Binance and send it to Kakfa producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50058ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from binance.client import Client\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "#  using API keys\n",
    "api_key = 'giuBTEvNmtfaSuPpCZfmF7uXzRYfKzk7sAwC4ezjB3KbfGLS30UnQMDxcxs15WSB'\n",
    "api_secret = 'SOmHSWFBuTa20grpf8r87c9qm9tym1oHkjktpu4mIwB9L08qvXW4W9HK7FSt1y6o'\n",
    "\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "def get_historical_data(symbol, interval, lookback):\n",
    "    \"\"\"\n",
    "    Fetch historical data from Binance for a given symbol and interval.\n",
    "    :param symbol: The trading pair symbol (e.g., 'BTCUSDT').\n",
    "    :param interval: The time interval for the data (e.g., '1m', '5m', '1h', '1d').\n",
    "    :param lookback: The lookback period for the data (e.g., '1 day ago UTC', '1 hour ago UTC').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        klines = client.get_historical_klines(symbol, interval, lookback)\n",
    "        return klines\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return None\n",
    "    \n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',  # Replace with your Kafka broker if different\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')  # Serialize to JSON bytes\n",
    ")\n",
    "\n",
    "topic = 'StreamQuant'\n",
    "\n",
    "i = 0\n",
    "\n",
    "keys = [\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "binance_data = get_historical_data('BTCUSDT', '1m', '6 hours ago UTC')\n",
    "\n",
    "print(\"Fetching initial data for 6 hours...\")\n",
    "\n",
    "for cryto_data in binance_data:\n",
    "    values = [int(cryto_data[0])] + [float(x) for x in cryto_data[1:]]\n",
    "    data_dict = dict(zip(keys, values))\n",
    "    producer.send(topic, value=data_dict)\n",
    "    print(f\"Sent {topic} - {i}: {data_dict}\")\n",
    "\n",
    "print(\"Streaming data every minute...\")\n",
    "\n",
    "while True:\n",
    "    i +=1\n",
    "    binance_data = get_historical_data('BTCUSDT', '1m', '1 minute ago UTC')\n",
    "    values = [int(binance_data[0][0])] + [float(x) for x in binance_data[0][1:]]\n",
    "    data_dict = dict(zip(keys, values))\n",
    "    producer.send(topic, value=data_dict)\n",
    "    print(f\"Sent {topic} - {i}: {data_dict}\")\n",
    "    time.sleep(60)  # Sleep for 1 minute before fetching the next data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5efd3",
   "metadata": {},
   "source": [
    "Run Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8b660",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ python3 binance_producer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc1ae02",
   "metadata": {},
   "source": [
    "### Create Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e1de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, DoubleType, StructField, LongType\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamQuant\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "schema = StructType([\n",
    "  StructField('timestamp', LongType()),\n",
    "  StructField('open', DoubleType()),\n",
    "  StructField('high', DoubleType()),\n",
    "  StructField('low', DoubleType()),\n",
    "  StructField('close', DoubleType()),\n",
    "  StructField('volume', DoubleType())\n",
    "])\n",
    "\n",
    "df_raw = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"StreamQuant\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "df_raw = df_raw.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df_raw = df_raw.withColumn(\"value\", F.from_json(df_raw.value, schema))\\\n",
    "    .select(\"value.*\")  \n",
    "\n",
    "df_raw = df_raw.withColumn(\"datetime\", F.from_unixtime(F.col(\"timestamp\") / 1000))\n",
    "\n",
    "df_raw.writeStream \\\n",
    " .format(\"parquet\") \\\n",
    " .option(\"path\", \"/tmp/stream_output/\") \\\n",
    " .option(\"checkpointLocation\", \"/tmp/stream_checkpoint/\") \\\n",
    " .outputMode(\"append\") \\\n",
    " .start() \\\n",
    " .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ef28b",
   "metadata": {},
   "source": [
    "Running Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa00c2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 ~/Realtime_Analytics/quant_consumer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed4c25",
   "metadata": {},
   "source": [
    "## Trading Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2dd1d",
   "metadata": {},
   "source": [
    "The strategy identifies potential buying and selling opportunities based on a combination of technical indicators: the Relative Strength Index (RSI) and moving average crossovers. A BUY signal is generated when the RSI is above 50 but below 70, and a bullish crossover occurs defined as the 20-period moving average crossing above the 50-period moving average. This suggests a strengthening upward trend that has not yet entered overbought territory. Conversely, a SELL signal is triggered when the RSI exceeds 70 and a bearish crossover is detected, where the 20-period moving average crosses below the 50-period moving average, indicating a possible reversal from overbought conditions. In the absence of these conditions, the strategy holds the position (HOLD), avoiding unnecessary trades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51355b0d",
   "metadata": {},
   "source": [
    "### 📌 Trading Strategy Summary\n",
    "\n",
    "* **Indicators Used**:\n",
    "\n",
    "  * RSI (Relative Strength Index)\n",
    "  * 20-period and 50-period Moving Averages (MA20 & MA50)\n",
    "\n",
    "* **Buy Signal**:\n",
    "\n",
    "  * RSI is between **50 and 70**\n",
    "  * **Bullish crossover**: MA20 crosses **above** MA50 \n",
    "\n",
    "* **Sell Signal**:\n",
    "\n",
    "  * RSI is **above 70**\n",
    "  * **Bearish crossover**: MA20 crosses **below** MA50 \n",
    "\n",
    "* **Hold**:\n",
    "\n",
    "  * All other cases — no action taken\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1cf767",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce067785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries import\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.types import StructType, DoubleType, StructField, LongType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from  pyspark.sql.functions import *\n",
    "\n",
    "# Define the Spark session and schema\n",
    "spark = SparkSession.builder.appName(\"TradingDashboard\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "schema = StructType([\n",
    "  StructField('timestamp', LongType()),\n",
    "  StructField('open', DoubleType()),\n",
    "  StructField('high', DoubleType()),\n",
    "  StructField('low', DoubleType()),\n",
    "  StructField('close', DoubleType()),\n",
    "  StructField('volume', DoubleType())\n",
    "])\n",
    "\n",
    "# Streamlit app\n",
    "st.set_page_config(page_title=\"📈 Real-Time Trading Dashboard\", layout=\"wide\")\n",
    "st.title(\"📊 Real-Time Trading with Candlestick, EMA, RSI & Signals\")\n",
    "\n",
    "DATA_DIR = \"/tmp/stream_output/\"\n",
    "# Refresh every 65 seconds\n",
    "REFRESH_INTERVAL = 65\n",
    "\n",
    "chart_placeholder = st.empty()\n",
    "\n",
    "def load_data():\n",
    "    files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".parquet\")]\n",
    "    if not files:\n",
    "        return spark.createDataFrame([], schema)\n",
    "\n",
    "    df = spark.read.parquet(f\"{DATA_DIR}*.parquet\")\n",
    "    df = df.dropDuplicates().orderBy(\"timestamp\")\n",
    "    return df\n",
    "\n",
    "def compute_indicators(df):\n",
    "    window_spec_20 = Window.orderBy(\"datetime\").rowsBetween(-19, 0)  \n",
    "    window_spec_50 = Window.orderBy(\"datetime\").rowsBetween(-49, 0)\n",
    "\n",
    "    df = df.withColumn(\"MA20\", avg(\"close\").over(window_spec_20))\n",
    "    df = df.withColumn(\"MA50\", avg(\"close\").over(window_spec_50))\n",
    "\n",
    "    window_spec = Window.orderBy(\"datetime\")\n",
    "    df = df.withColumn(\"prev_close\", lag(\"close\", 1).over(window_spec))\n",
    "    df = df.withColumn(\"change\", col(\"close\") - col(\"prev_close\"))\n",
    "    df = df.withColumn(\"gain\", when(col(\"change\") > 0, col(\"change\")).otherwise(0))\n",
    "    df = df.withColumn(\"loss\", when(col(\"change\") < 0, -col(\"change\")).otherwise(0))\n",
    "\n",
    "    rsi_window = Window.orderBy(\"datetime\").rowsBetween(-13, 0)\n",
    "    df = df.withColumn(\"avg_gain\", avg(\"gain\").over(rsi_window))\n",
    "    df = df.withColumn(\"avg_loss\", avg(\"loss\").over(rsi_window))\n",
    "\n",
    "    df = df.withColumn(\"rs\", col(\"avg_gain\") / col(\"avg_loss\"))\n",
    "    df = df.withColumn(\"RSI\", 100 - (100 / (1 + col(\"rs\"))))\n",
    "\n",
    "    df = df.withColumn(\"prev_MA20\", lag(\"MA20\", 1).over(window_spec))\n",
    "    df = df.withColumn(\"prev_MA50\", lag(\"MA50\", 1).over(window_spec))\n",
    "\n",
    "    df = df.withColumn(\"ma_crossover_up\", \n",
    "                   when((col(\"prev_MA20\") < col(\"prev_MA50\")) & (col(\"MA20\") > col(\"MA50\")), lit(1)).otherwise(0))\n",
    "\n",
    "    df = df.withColumn(\"ma_crossover_down\", \n",
    "                   when((col(\"prev_MA20\") > col(\"prev_MA50\")) & (col(\"MA20\") < col(\"MA50\")), lit(1)).otherwise(0))\n",
    "    \n",
    "    df = df.withColumn(\"signal\", \n",
    "                    when((col(\"RSI\") > 50) & (col(\"ma_crossover_up\") == 1) & (col(\"RSI\") < 70), lit(\"BUY\"))\n",
    "                    .when((col(\"RSI\") > 70) & (col(\"ma_crossover_down\") == 1), lit(\"SELL\"))\n",
    "                    .otherwise(\"HOLD\")\n",
    "                    )                       \n",
    "    df = df.drop(\"prev_close\", \"gain\", \"loss\", \"avg_gain\", \"avg_loss\", \"rs\", \"change\", \"ma_crossover_down\", \"ma_crossover_up\", \"prev_MA20\", \"prev_MA50\")\n",
    "    df = df.orderBy(col(\"timestamp\").desc()).limit(240)\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_price_chart(df):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Candlestick\n",
    "    fig.add_trace(go.Candlestick(\n",
    "        x=df[\"datetime\"],\n",
    "        open=df[\"open\"],\n",
    "        high=df[\"high\"],\n",
    "        low=df[\"low\"],\n",
    "        close=df[\"close\"],\n",
    "        name=\"Candlestick\"\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df[\"datetime\"], y=df[\"MA20\"],\n",
    "        line=dict(color=\"green\", width=2),\n",
    "        name=\"MA_20\"\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df[\"datetime\"], y=df[\"MA50\"],\n",
    "        line=dict(color=\"red\", width=2),\n",
    "        name=\"MA_50\"\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(title=\"📈 Price with MA + Signals\",\n",
    "                      xaxis_title=\"Time\", yaxis_title=\"Price\",\n",
    "                      xaxis_rangeslider_visible=False,\n",
    "                      height=600)\n",
    "    return fig\n",
    "\n",
    "def plot_rsi(df):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df[\"datetime\"], y=df[\"RSI\"], line=dict(color=\"blue\")))\n",
    "    fig.add_hline(y=70, line_dash=\"dash\", line_color=\"red\")\n",
    "    fig.add_hline(y=30, line_dash=\"dash\", line_color=\"green\")\n",
    "    fig.update_layout(title=\"📉 RSI\", height=300)\n",
    "    return fig\n",
    "\n",
    "def plot_total_asset(df):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df[\"timestamp\"], y=df[\"total_value\"], line=dict(color=\"blue\")))\n",
    "    fig.add_hline(y=10000, line_dash=\"dash\", line_color=\"red\")\n",
    "    fig.update_layout(title=\"📉 Total Asset\", height=300)\n",
    "    return fig\n",
    "\n",
    "crypto = 0\n",
    "budget = 10000\n",
    "\n",
    "def trading_strategy(df):\n",
    "    global budget, crypto\n",
    "    latest = df.iloc[0]\n",
    "    price = latest[\"close\"]\n",
    "    signal = latest[\"signal\"]\n",
    "    datetime_str = latest[\"datetime\"]\n",
    "\n",
    "    if signal == \"BUY\" and crypto == 0:\n",
    "        amount = budget // price\n",
    "        crypto += amount\n",
    "        budget -= amount * price\n",
    "        action = \"BUY\"\n",
    "    elif signal == \"SELL\" and crypto > 0:\n",
    "        amount = crypto\n",
    "        crypto -= amount\n",
    "        budget += amount * price\n",
    "        action = \"SELL\"\n",
    "    else:\n",
    "        action = \"HOLD\"\n",
    "\n",
    "    log = pd.DataFrame([{   \"timestamp\": datetime_str, \n",
    "                            \"action\": action,\n",
    "                            \"crypto\": crypto,\n",
    "                            \"price\": price,\n",
    "                            \"budget\": budget,   \n",
    "                            \"total_value\": budget + (crypto * price)                        \n",
    "                        }])\n",
    "    \n",
    "    return log\n",
    "\n",
    "price_chart_container = st.empty()\n",
    "rsi_chart_container = st.empty()\n",
    "lastest_container = st.empty()\n",
    "pnl = st.empty()\n",
    "\n",
    "portfolio_log = pd.DataFrame(columns=[\"timestamp\", \"action\", \"crypto\", \"price\", \"budget\", \"total_value\"])\n",
    "\n",
    "while True:\n",
    "    df = load_data()\n",
    "\n",
    "    if df.count() == 0:\n",
    "        st.warning(\"⏳ Waiting for Spark to write streaming data...\")\n",
    "    else:\n",
    "        df = compute_indicators(df)\n",
    "        df = df.toPandas()\n",
    "\n",
    "        log = trading_strategy(df)\n",
    "        portfolio_log = pd.concat([portfolio_log, log], ignore_index=True)\n",
    "\n",
    "        # Plot charts\n",
    "        price_fig = plot_price_chart(df)\n",
    "        pnl_fig = plot_total_asset(portfolio_log)\n",
    "        rsi_fig = plot_rsi(df)\n",
    "\n",
    "        # Layout\n",
    "        price_chart_container.plotly_chart(price_fig, use_container_width=True)\n",
    "        pnl.plotly_chart(pnl_fig, use_container_width=True)\n",
    "        rsi_chart_container.plotly_chart(rsi_fig, use_container_width=True)\n",
    "\n",
    "        with lastest_container.expander(\"📄 Latest Data\"):\n",
    "            lastest_container.dataframe(df.sort_values(\"datetime\", ascending=False).tail(10))\n",
    "\n",
    "    time.sleep(REFRESH_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada52d5",
   "metadata": {},
   "source": [
    "For running the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b25c6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ streamlit run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b36360",
   "metadata": {},
   "source": [
    "The app will refresh every 65 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ba4db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bcce704",
   "metadata": {},
   "source": [
    "Stop Kafka and Zookeeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07968d7c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ cd /usr/local/kafka\n",
    "$ sudo bin/zookeeper-server-stop.sh\n",
    "$ sudo bin/kafka-server-stop.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
